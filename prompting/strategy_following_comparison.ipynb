{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading multiple attention files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "bases = glob('outputs/exp1*')\n",
    "bases += glob('outputs/exp2*')\n",
    "bases += glob('outputs/exp3*')\n",
    "\n",
    "exp_files = [glob(os.path.join(b, '*_attentions.pkl')) for b in bases]\n",
    "print([(b, len(e)) for b, e in zip(bases, exp_files)])\n",
    "\n",
    "json_files = [[p.replace('_attentions.pkl', '.json') for p in files] for files in exp_files]\n",
    "\n",
    "exp_files[-1][:3], json_files[-1][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting attention on strategy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from analyze_attention_weights import get_average_attention_over_sequence\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-70b-chat-hf')\n",
    "\n",
    "def get_attention_per_strategy(files, exp_name) -> List[Dict]:\n",
    "    data = []\n",
    "    filtered_files = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(file, 'rb') as f:\n",
    "                data.append(pickle.load(f))\n",
    "                filtered_files.append(file)\n",
    "        except Exception as e:\n",
    "            print(f\"couldn't load {file}.\")\n",
    "            raise e\n",
    "\n",
    "    res = []\n",
    "    for example, p in tqdm(zip(data, filtered_files)):\n",
    "        if example is None:\n",
    "            continue\n",
    "        for strategy, (tokens, attentions) in example.items():\n",
    "            strategy_str = f'\"{strategy}\"'\n",
    "            attn = get_average_attention_over_sequence(attentions, tokens, sequence=strategy_str, tokenizer=tokenizer)\n",
    "            res.append({\n",
    "                'strategy': strategy,\n",
    "                'strategy_attn': attn,\n",
    "                'method': exp_name,\n",
    "                'path': p\n",
    "            })\n",
    "            \n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "attns = [get_attention_per_strategy(files, exp_name) for files, exp_name in zip(exp_files, [b.split('/')[-1] for b in bases])]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize sra for different model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_theme(style='whitegrid') \n",
    "\n",
    "all_attns = [a for attn in attns for a in attn]\n",
    "\n",
    "df = pd.DataFrame(all_attns)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_template_type(value):\n",
    "    if 'full' in value:\n",
    "        return 'standard'\n",
    "    elif 'c1_hf' in value:\n",
    "        return 'c1_hf'\n",
    "    elif 'c3_hf' in value:\n",
    "        return 'c3_hf'\n",
    "    elif 'c5_hf' in value:\n",
    "        return 'c5_hf'\n",
    "    elif 'c1_hl' in value:\n",
    "        return 'c1_hl'\n",
    "    elif 'c3_hl' in value:\n",
    "        return 'c3_hl'\n",
    "    elif 'c5_hl' in value:\n",
    "        return 'c5_hl'\n",
    "    else:\n",
    "        print(value)\n",
    "        raise Exception()\n",
    "\n",
    "def add_model_size(value):\n",
    "    if '7b' in value:\n",
    "        return 'llama-7b-chat'\n",
    "    elif '13b' in value:\n",
    "        return 'llama-13b-chat'\n",
    "    elif '70b' in value:\n",
    "        return 'llama-70b-chat'\n",
    "    else:\n",
    "        print(value)\n",
    "        raise Exception()\n",
    "\n",
    "\n",
    "df['template type'] = df['method'].apply(add_template_type)\n",
    "df['model size'] = df['method'].apply(add_model_size)\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(df, x='model size', y='strategy_attn', hue='template type')\n",
    "plt.ylabel('SRA')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "all_attns = [a for attn in attns for a in attn]\n",
    "df = pd.DataFrame(all_attns)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "df['template type'] = df['method'].apply(add_template_type)\n",
    "sns.barplot(df[df['method'].str.contains('exp1')], x='strategy', y='strategy_attn', hue='template type')\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.ylabel('SRA')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## measuring attention vs number of utterances in history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get length of inputs per attention outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "attn_lens = []\n",
    "all_attns = [a for attn in attns for a in attn]\n",
    "\n",
    "for example in tqdm(all_attns):\n",
    "    json_path = example['path'].replace('_attentions.pkl', '.json')\n",
    "    \n",
    "    with open(json_path, 'r') as f:\n",
    "        json_d = json.load(f)\n",
    "    n_turns = len(json_d['dialog'])\n",
    "    d = example.copy()\n",
    "    d['n_turns'] = n_turns\n",
    "    attn_lens.append(d)\n",
    "\n",
    "attn_lens[-1]\n",
    "\n",
    "df = pd.DataFrame(attn_lens)\n",
    "df['template type'] = df['method'].apply(add_template_type)\n",
    "df['model name'] = df['method'].apply(add_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df['template type'] = df['method'].apply(add_template_type)\n",
    "df['model size'] = df['method'].apply(add_model_size)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.boxplot(df[(df['template type'] == 'c1_hf') | (df['template type'] == 'standard')], x='n_turns', y='strategy_attn', hue='template type')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('SRA')\n",
    "plt.xlabel('turn')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n",
    "\n",
    "# Color palette\n",
    "template_types = list(df['template type'].unique())\n",
    "palette = sns.color_palette('husl', len(template_types))\n",
    "\n",
    "model_names = list(df['model name'].unique())\n",
    "for ax, model_name in zip(axs, model_names):\n",
    "    cur_df = df[df['model name'] == model_name]\n",
    "    ax.set_title(model_name)\n",
    "    # Iterate over each group to plot\n",
    "    for i, group in enumerate(template_types):\n",
    "        subset = cur_df[cur_df['template type'] == group]\n",
    "        sns.regplot(x='n_turns', y='strategy_attn', data=subset, ax=ax, label=group, color=palette[i], scatter_kws={'s': 50})\n",
    "    ax.set_ylabel('SRA')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.4, hspace=0.4)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# measuring predictability of responses for each prompting scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "bases = glob('outputs/exp1*')\n",
    "\n",
    "exp_files = [glob(os.path.join(b, '*_attentions.pkl')) for b in bases]\n",
    "json_files = [[p.replace('_attentions.pkl', '.json') for p in files] for files in exp_files]\n",
    "\n",
    "\n",
    "def load_dataset(files):\n",
    "    ds = []\n",
    "    for p in files:\n",
    "        with open(p, 'r') as f:\n",
    "            d = json.load(f)\n",
    "            ds.append(d)\n",
    "    return ds\n",
    "\n",
    "datasets = [load_dataset(fs) for fs in json_files]\n",
    "\n",
    "for ds in datasets:\n",
    "    print(\"size of ds: \", len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert to seq classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_seq_classification_ds(ds):\n",
    "    seqs = []\n",
    "    labels = []\n",
    "    turns = []\n",
    "    wrong_format = 0\n",
    "    total = 0\n",
    "    \n",
    "    for d in ds:\n",
    "        if d is None:\n",
    "            print(\"Ran into empty json\")\n",
    "            continue\n",
    "        for strategy, resp in d['responses'].items():\n",
    "            seq_txt = resp.lower()\n",
    "            if 'assistant: ' not in seq_txt:\n",
    "                wrong_format += 1\n",
    "                seq_txt = 'assistant: ' + seq_txt\n",
    "            \n",
    "            assert 'assistant: ' in seq_txt, seq_txt\n",
    "\n",
    "            seq_txt = seq_txt.split('assistant: ')[-1]\n",
    "\n",
    "            #remove mentions of the strategy in text\n",
    "            seq_txt = seq_txt.replace(strategy.lower(), \"\")\n",
    "            seqs.append(seq_txt)\n",
    "            labels.append(strategy)\n",
    "            turns.append(len(d['dialog']))\n",
    "            total += 1\n",
    "\n",
    "    print(f\"{wrong_format}/{total} had wrong format\")\n",
    "    return seqs, labels, turns\n",
    "\n",
    "datasets_xy = [to_seq_classification_ds(d) for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def train_and_report(ds_x, ds_y):\n",
    "    vectorizer = CountVectorizer(stop_words=english_stopwords, lowercase=True, max_df=0.9, ngram_range=(2, 3))\n",
    "    X = vectorizer.fit_transform(ds_x)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(ds_y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, encoded_labels, test_size=0.2)\n",
    "    \n",
    "    model = LogisticRegressionCV(max_iter=200, cv=4)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print('accuracy: ', acc)\n",
    "    return acc\n",
    "\n",
    "\n",
    "names = [b.split('/')[-1] for b in bases]\n",
    "accs = []\n",
    "for name, ds in zip(names, datasets_xy):\n",
    "    print(name)\n",
    "    accs.append(train_and_report(ds[0], ds[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('pred_acc_7b.pkl', 'wb') as f:\n",
    "#     pickle.dump((names, accs), f)\n",
    "\n",
    "# with open('pred_acc_13b.pkl', 'wb') as f:\n",
    "#     pickle.dump((names, accs), f)\n",
    "\n",
    "with open('pred_acc_70b.pkl', 'wb') as f:\n",
    "    pickle.dump((names, accs), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine results and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pred_acc_7b.pkl', 'rb') as f:\n",
    "    names_7b, accs_7b = pickle.load(f)\n",
    "\n",
    "with open('pred_acc_13b.pkl', 'rb') as f:\n",
    "    names_13b, accs_13b = pickle.load(f)\n",
    "\n",
    "with open('pred_acc_70b.pkl', 'rb') as f:\n",
    "    names_70b, accs_70b = pickle.load(f)\n",
    "\n",
    "names = names_7b + names_13b + names_70b\n",
    "accs = accs_7b + accs_13b + accs_70b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_theme(style='whitegrid') \n",
    "\n",
    "df = pd.DataFrame({'name': names, 'accuracy':accs})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_template_type(value):\n",
    "    if 'full' in value:\n",
    "        return 'standard'\n",
    "    elif 'c1_hf' in value:\n",
    "        return 'c1_hf'\n",
    "    elif 'c3_hf' in value:\n",
    "        return 'c3_hf'\n",
    "    elif 'c5_hf' in value:\n",
    "        return 'c5_hf'\n",
    "    elif 'c1_hl' in value:\n",
    "        return 'c1_hl'\n",
    "    elif 'c3_hl' in value:\n",
    "        return 'c3_hl'\n",
    "    elif 'c5_hl' in value:\n",
    "        return 'c5_hl'\n",
    "    else:\n",
    "        print(value)\n",
    "        raise Exception()\n",
    "\n",
    "def add_model_size(value):\n",
    "    if '7b' in value:\n",
    "        return 'llama-7b-chat'\n",
    "    elif '13b' in value:\n",
    "        return 'llama-13b-chat'\n",
    "    elif '70b' in value:\n",
    "        return 'llama-70b-chat'\n",
    "    else:\n",
    "        print(value)\n",
    "        raise Exception()\n",
    "\n",
    "\n",
    "df['template type'] = df['name'].apply(add_template_type)\n",
    "df['model size'] = df['name'].apply(add_model_size)\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(df, x='model size', y='accuracy', hue='template type')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier on semantic embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = glob('outputs/exp1*/response_encodings.pkl')\n",
    "\n",
    "\n",
    "datasets_xy = []\n",
    "for p in dataset_paths:\n",
    "    with open(p, 'rb') as f:\n",
    "        ds = pickle.load(f)\n",
    "        print(len(ds[0]))\n",
    "        datasets_xy.append(ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "names = [re.findall('exp\\d+_(.+)/', x)[0] for x in dataset_paths]\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def train_and_report(ds_x, ds_y):\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(ds_y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(ds_x, encoded_labels, test_size=0.2)\n",
    "    \n",
    "    model = LogisticRegressionCV(max_iter=200, cv=4)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print('accuracy: ', acc)\n",
    "    return acc\n",
    "\n",
    "\n",
    "accs = []\n",
    "for name, ds in zip(names, datasets_xy):\n",
    "    print(name)\n",
    "    accs.append(train_and_report(ds[0], ds[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# with open('pred_acc_sbert_7b.pkl', 'wb') as f:\n",
    "#     pickle.dump((names, accs), f)\n",
    "\n",
    "# with open('pred_acc_sbert_13b.pkl', 'wb') as f:\n",
    "#     pickle.dump((names, accs), f)\n",
    "\n",
    "with open('pred_acc_sbert_70b.pkl', 'wb') as f:\n",
    "    pickle.dump((names, accs), f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combine results and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pred_acc_sbert_7b.pkl', 'rb') as f:\n",
    "    names_7b, accs_7b = pickle.load(f)\n",
    "\n",
    "with open('pred_acc_sbert_13b.pkl', 'rb') as f:\n",
    "    names_13b, accs_13b = pickle.load(f)\n",
    "\n",
    "with open('pred_acc_sbert_70b.pkl', 'rb') as f:\n",
    "    names_70b, accs_70b = pickle.load(f)\n",
    "\n",
    "names = names_7b + names_13b + names_70b\n",
    "accs = accs_7b + accs_13b + accs_70b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_theme(style='whitegrid') \n",
    "\n",
    "df = pd.DataFrame({'name': names, 'accuracy':accs})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_template_type(value):\n",
    "    if 'full' in value:\n",
    "        return 'standard'\n",
    "    elif 'c1_hf' in value:\n",
    "        return 'c1_hf'\n",
    "    elif 'c3_hf' in value:\n",
    "        return 'c3_hf'\n",
    "    elif 'c5_hf' in value:\n",
    "        return 'c5_hf'\n",
    "    elif 'c1_hl' in value:\n",
    "        return 'c1_hl'\n",
    "    elif 'c3_hl' in value:\n",
    "        return 'c3_hl'\n",
    "    elif 'c5_hl' in value:\n",
    "        return 'c5_hl'\n",
    "    else:\n",
    "        print(value)\n",
    "        raise Exception()\n",
    "\n",
    "def add_model_size(value):\n",
    "    if '7b' in value:\n",
    "        return 'llama-7b-chat'\n",
    "    elif '13b' in value:\n",
    "        return 'llama-13b-chat'\n",
    "    elif '70b' in value:\n",
    "        return 'llama-70b-chat'\n",
    "    else:\n",
    "        print(value)\n",
    "        raise Exception()\n",
    "\n",
    "\n",
    "df['template type'] = df['name'].apply(add_template_type)\n",
    "df['model size'] = df['name'].apply(add_model_size)\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(df, x='model size', y='accuracy', hue='template type')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combined plot for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pred_acc_sbert_7b.pkl', 'rb') as f:\n",
    "    names_7b, accs_7b = pickle.load(f)\n",
    "\n",
    "with open('pred_acc_sbert_13b.pkl', 'rb') as f:\n",
    "    names_13b, accs_13b = pickle.load(f)\n",
    "\n",
    "with open('pred_acc_sbert_70b.pkl', 'rb') as f:\n",
    "    names_70b, accs_70b = pickle.load(f)\n",
    "\n",
    "names = names_7b + names_13b + names_70b\n",
    "accs = accs_7b + accs_13b + accs_70b\n",
    "predictors = ['semantic features']*len(names)\n",
    "\n",
    "with open('pred_acc_7b.pkl', 'rb') as f:\n",
    "    names_7b, accs_7b = pickle.load(f)\n",
    "\n",
    "with open('pred_acc_13b.pkl', 'rb') as f:\n",
    "    names_13b, accs_13b = pickle.load(f)\n",
    "\n",
    "with open('pred_acc_70b.pkl', 'rb') as f:\n",
    "    names_70b, accs_70b = pickle.load(f)\n",
    "\n",
    "names += (names_7b + names_13b + names_70b)\n",
    "accs += (accs_7b + accs_13b + accs_70b)\n",
    "predictors += ['lexical features']*len((names_7b + names_13b + names_70b))\n",
    "\n",
    "len(names), len(accs), len(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set_theme(style='whitegrid') \n",
    "\n",
    "df = pd.DataFrame({'name': names, 'accuracy':accs, 'predictor': predictors})\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def add_template_type(value):\n",
    "    if 'full' in value:\n",
    "        return 'standard'\n",
    "    elif 'c1_hf' in value:\n",
    "        return 'c1_hf'\n",
    "    elif 'c3_hf' in value:\n",
    "        return 'c3_hf'\n",
    "    elif 'c5_hf' in value:\n",
    "        return 'c5_hf'\n",
    "    elif 'c1_hl' in value:\n",
    "        return 'c1_hl'\n",
    "    elif 'c3_hl' in value:\n",
    "        return 'c3_hl'\n",
    "    elif 'c5_hl' in value:\n",
    "        return 'c5_hl'\n",
    "    else:\n",
    "        print(value)\n",
    "        raise Exception()\n",
    "\n",
    "def add_model_size(value):\n",
    "    if '7b' in value:\n",
    "        return 'llama-7b-chat'\n",
    "    elif '13b' in value:\n",
    "        return 'llama-13b-chat'\n",
    "    elif '70b' in value:\n",
    "        return 'llama-70b-chat'\n",
    "    else:\n",
    "        print(value)\n",
    "        raise Exception()\n",
    "\n",
    "\n",
    "df['template type'] = df['name'].apply(add_template_type)\n",
    "df['model size'] = df['name'].apply(add_model_size)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "sns.barplot(data=df[df['predictor']=='lexical features'], x='model size', y='accuracy', hue='template type', ax=axes[0])\n",
    "axes[0].set_title('LR on lexical features')\n",
    "\n",
    "sns.barplot(data=df[df['predictor']=='semantic features'], x='model size', y='accuracy', hue='template type', ax=axes[1])\n",
    "axes[1].set_title('LR on semantic features')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyzing LR coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=english_stopwords, lowercase=True, max_df=0.9, ngram_range=(2, 3))\n",
    "\n",
    "ds_x, ds_y = datasets_xy[0][0], datasets_xy[0][1]\n",
    "\n",
    "X = vectorizer.fit_transform(ds_x)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(ds_y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, encoded_labels, test_size=0.2)\n",
    "\n",
    "model = LogisticRegressionCV(max_iter=300, cv=4)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    coefficients = model.coef_[i]\n",
    "    strategy = label_encoder.inverse_transform([i])[0]\n",
    "    \n",
    "    # Step 4: Map coefficients to words\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    word_coefficient_tuples = list(zip(feature_names, coefficients))\n",
    "    \n",
    "    # Step 5: Sort and display\n",
    "    sorted_word_coefficients = sorted(word_coefficient_tuples, key=lambda x: x[1], reverse=True)\n",
    "    print(\"*\"*100)\n",
    "    print(\"strategy: \", strategy)\n",
    "    print(\"*\"*100)\n",
    "    for word, score in sorted_word_coefficients[:10]:\n",
    "        print(f\"{word} -> {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
